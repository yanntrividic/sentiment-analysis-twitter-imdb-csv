<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>src.model.requests_handler API documentation</title>
<meta name="description" content="Created on Mar 5, 2021 …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.model.requests_handler</code></h1>
</header>
<section id="section-intro">
<p>Created on Mar 5, 2021</p>
<p>@author: yann</p>
<p>Module that contains the RequestsHandler class.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Created on Mar 5, 2021

@author: yann

Module that contains the RequestsHandler class.
&#39;&#39;&#39;

from collections import defaultdict
import re
from threading import Thread
import time
import traceback

from imdb_extraction import requetes_imdb
from local_extraction import LocalImporter
from network_utils import (is_connected_internet, TIMEOUT_DURATION,
                           is_googletrans_api_up, is_api_up, is_detectlanguage_api_up, is_tweepy_up, is_tweepy_api_key_valid)
from nlp_utils import convert_lang_format
from request_output import RequestOutput
from selenium.webdriver import firefox, chrome
from translator import translate_df, is_valid_target_lang
from twitter_extraction import requetes_twitter
from voting_system import (get_imdb_short_reviews_vc, get_imdb_allocine_vc,
                           get_twitter_en_vc, get_twitter_fr_vc, VoteClassifier)

import numpy as np
import pandas as pd

# Allowed keys in the request (followed by a KEY_SEPARATOR) :
SEARCH_KEYS = [&#39;search&#39;, &#39;film&#39;, &#39;user&#39;, &#39;hashtag&#39;, &#39;lang&#39;, &#39;target&#39;, &#39;src&#39;, &#39;enddate&#39;, &#39;begindate&#39;, &#39;type&#39;,
               &#39;year&#39;, &#39;id&#39;, &#39;maxentries&#39;, &#39;colname&#39;]

METADATA_KEYS = [&#39;foundentries&#39;]

# Allowed characters in the request&#39; attributes (A to Z lower case and capitals, 0 to 9, accentuated characters)
ALLOWED_CHARS = &#39;a-zA-Z0-9À-ÿ\-_&#39;
FILM_TITLE_ALLOWED_CHARS = &#39;:,\&#39;\.!\?() &#39;
FILM_YEAR_PATTERN = r&#39;^(19|20)\d{2}$&#39;
IMDB_ID_TT_PATTERN = r&#39;^(tt\d{7}|tt\d{5})$&#39;
IMDB_ID_NB_PATTERN = r&#39;^(\d{5}|\d{7})$&#39;

DEFAULT_LANG = &#39;en&#39;

DEFAULT_MAX_ENTRIES = 50
MAX_ENTRIES_PATTERN = r&#39;^([1-9][0-9]{0,2}|1000)$&#39;

# Operators allowed
HASHTAG_KEY = &#39;#&#39;  # leading char for hashtag keywords
FILM_KEY = &#39;\&#34;&#39;  # film title have to be entered between two apostrophes
USER_KEY = &#39;@&#39;  # leading char for user keywords
KEY_SEPARATOR = &#39;:&#39;  # separator between a search key and a search value
INTERSECTION_SEPARATOR = &#39;,&#39;  # separator between two sub-requests
UNION_SEPARATOR = &#39; &#39;  # separator between keywords and tuples of searchkeys/values
TMP_FILM_OPERATOR = &#39;*&#39;  # character to locate film_titles

POL_COLUMN_NAME = &#34;polarity&#34;  # name of the column that will contains the polarity data


class RequestsHandler:
    &#39;&#39;&#39;
    Main class of the package. This is where the requests from the control will be interpreted. Every flow of data
    will go through this class to go in or out of the model package.
    &#39;&#39;&#39;

    def __init__(self):
        &#39;&#39;&#39;
        Constructor
        &#39;&#39;&#39;

        self.str_request = &#34;&#34;  # the unprocessed request
        self.processed_data = None  # the data to return to the controller
        # self.api_keys = None #might not be useful as an attribute
        self.interpreted_request = {}  # will contain the list of dict extracted from the original str

        # Data extractors
        # self.imdb = new instance of extractor, with the argument read_api_key(&#39;imdb&#39;)
        # self.twitter = new instance of extractor, with the argument read_api_key(&#39;twitter&#39;)
        self.local_data = None  # new instance of local extractor
        self.imdb_webdriver = None

        # Classifiers
        # self.vc_en_imdb = get_imdb_long_reviews_vc()  # instantiates the VoteClassifier for IMDb
        self.vc_en_twitter = VoteClassifier(&#34;Twitter (en)&#34;)
        self.vc_fr_twitter = VoteClassifier(&#34;Twitter (fr)&#34;)
        self.vc_en_imdb = VoteClassifier(&#34;IMDb (en)&#34;)
        self.vc_fr_imdb = VoteClassifier(&#34;IMDb (fr)&#34;)

    def load_classifiers(self, multithreading=True):
        time.sleep(0.1)  # this line allows the time for the GUI to load correctly
        if multithreading:  # with multithreading
            vcs = [(self.vc_en_twitter, get_twitter_en_vc),
                   (self.vc_fr_twitter, get_twitter_fr_vc),
                   (self.vc_en_imdb, get_imdb_short_reviews_vc),
                   (self.vc_fr_imdb, get_imdb_allocine_vc)]

            threads = []

            for vc, get in vcs:
                t = Thread(target=get, args=[vc])
                t.start()
                threads.append(t)

            for t in threads:
                t.join()

        else:  # without multithreading
            get_twitter_en_vc(self.vc_en_twitter)
            get_imdb_short_reviews_vc(self.vc_en_imdb)
            get_twitter_fr_vc(self.vc_fr_twitter)

    def __repr__(self):
        s = &#34;\nRequestsHandler state:\n&#34;
        s += &#34;\t* Raw request: \&#34;&#34; + self.str_request + &#34;\&#34;\n&#34;
        s += &#34;\t* Interpreted request: &#34; + str(self.interpreted_request) + &#34;\n&#34;
        s += &#34;\t* Imported data? &#34; + (&#34;No&#34; if self.local_data is None else &#34;Yes&#34;) + &#34;.\n&#34;
        s += &#34;\t* Has the data been processed? &#34; + (&#34;No&#34; if self.processed_data is None else &#34;Yes:\n&#34; +
                                                    str(self.processed_data)) + &#34;.\n&#34;
        s += (&#34;\t* Webdriver: &#34; + (&#34;Firefox&#34; if isinstance(self.imdb_webdriver, firefox.webdriver.WebDriver) else &#34;&#34;) +
              (&#34;Chrome&#34; if isinstance(self.imdb_webdriver, chrome.webdriver.WebDriver) else &#34;&#34;) +
              (&#34;None&#34; if not self.imdb_webdriver else &#34;&#34;)) + &#34;.\n\n&#34;
        return s

    def __interpret_request(self, request):
        if self.__check_basic_requirements(request):  # raises exceptions for basic invalidity
            self.str_request = request  # if there is no obvious errors, then it is passed as a value of the attribute
        self.__parse_request()
        self.__post_process_request()

        # checks semantic validity
        self.__validate_semantic()
        return True

    def __check_basic_requirements(self, req):
        &#39;&#39;&#39;Checks if the parameter of the method meets the basic requirements to be a valid request.
        The request is a str, it is not empty and contains only valid characters.
        &#39;&#39;&#39;

        if not(isinstance(req, str)):  # is a str
            raise RequestSyntaxError(req, &#34;The request is expected to be a str.&#34;)

        if len(req) &lt; 1:  # is not empty
            raise RequestSyntaxError(req, &#34;The request is empty.&#34;)

        if not(self.__match_regex(req)):  # contains the expected characters
            raise RequestSyntaxError(req, &#34;The request doesn&#39;t meet the expected syntax (invalid characters).&#34; +
                                     &#34;Please consult our User Manual.&#34;)

        return True

    def __match_regex(self, req):
        &#39;&#39;&#39;Method using a regular expression to check if the syntax of the request matches the requirements expected.
        &#39;&#39;&#39;

        basic_expression = ALLOWED_CHARS + HASHTAG_KEY + USER_KEY + KEY_SEPARATOR + INTERSECTION_SEPARATOR + UNION_SEPARATOR
        film_title_expression = ALLOWED_CHARS + FILM_TITLE_ALLOWED_CHARS  # film titles are treated separately

        pattern = r&#39;([{0}]+|&#34;[{1}]+&#34;+)&#39;.format(basic_expression, film_title_expression)

        regex = re.compile(pattern)
        results = regex.findall(req)  # finds every matching substring, treats separately film titles and the rest
        # print(&#34;result=&#34; + str(result))

        if len(results) &lt; 1:
            raise RequestSyntaxError(req, &#34;The request is empty.&#34;)

        length_sum = 0
        for result in results:
            for r in result:
                length_sum += len(r)

        # if the sum of the lengths is equal to what findall has found, then it matched
        # otherwise, there is an error
        return length_sum == len(req)

    def __find_films_and_replace(self):
        &#39;&#39;&#39;Films titles are tricky to deal with in this project since they can use a lot of different characters.
        We have to treat them apart from the rest of the str.
        &#39;&#39;&#39;
        film_titles = []
        if FILM_KEY not in self.str_request:  # if no film title is detected in the original request
            return film_titles, self.str_request  # then nothing is done

        # we detect the film titles with their by looking at quotations marks.
        pattern = r&#39;&#34;[{}]+&#34;&#39;.format(ALLOWED_CHARS + FILM_TITLE_ALLOWED_CHARS)
        regex = re.compile(pattern)
        film_titles = regex.findall(self.str_request)

        tmp_request = self.str_request

        for i in range((len(film_titles))):
            # since titles can hold a broader range of characters, we have to replace them with a special key
            tmp_request = tmp_request.replace(film_titles[i], TMP_FILM_OPERATOR)
            film_titles[i] = film_titles[i].replace(FILM_KEY, &#39;&#39;)  # removes the quotation mark from the stored value

        return (film_titles, tmp_request)

    def __parse_request(self):
        &#39;&#39;&#39;Method that naively goes through the raw string to parse it into a dict.
        Does some basic filtering in order to raise errors if needed.
        &#39;&#39;&#39;

        film_titles, tmp_request = self.__find_films_and_replace()  # removes film titles to deal with tricky characters
        tmp_request = tmp_request.lower()  # only the tmp_request is turned to lowercase
        films_index = 0  # index to keep count of where we are in the film&#39;s list
        intersect_requests = tmp_request.split(INTERSECTION_SEPARATOR)  # splits into several sub-requests

        if len(intersect_requests) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;Multiple requests are not supported at the time.&#34; +
                                     &#34; It will be supported in a future update.&#34;)

        tmp = []
        # by using a temporary variable, we ensure that the interpreted_request will only be updated if
        # the request is fully valid

        for req in intersect_requests:
            d = defaultdict(list)  # new empty dict of lists for this request

            if req == len(req) * UNION_SEPARATOR:  # when the request only contains spaces
                raise RequestSyntaxError(req, &#34;One of the subrequest is only made of &#39;&#34; + UNION_SEPARATOR + &#34;&#39;.&#34;)

            if len(req) == 0:  # or if the sequence &#39;,,&#39; is met
                raise RequestSyntaxError(req, &#34;One of the subrequest is empty.&#34;)

            union_requests = req.split(UNION_SEPARATOR)  # splits into keywords to interpret

            for u_req in union_requests:

                cut = u_req.split(KEY_SEPARATOR)

                if len(cut) == 2:
                    key = None
                    for search_key in SEARCH_KEYS:  # we verify if the key specified is a known key
                        if cut[0] == search_key:
                            key = search_key

                    if key is None or not cut[1]:  # if the word preceding the KEY_SEPARATOR isn&#39;t in SEARCH_KEYS then :
                        raise RequestSyntaxError(req, &#34;The request contains invalid filters.&#34;)
                    else:
                        if key == &#39;lang&#39;:
                            try:
                                cut = convert_lang_format(cut[1])
                            except ValueError as e:
                                raise RequestSyntaxError(req, e)
                            d[key].append(cut)

                        elif key == &#39;year&#39;:
                            year_pattern = re.compile(FILM_YEAR_PATTERN)
                            if re.match(year_pattern, cut[1]):
                                d[key].append(cut[1])
                            else:
                                raise RequestSyntaxError(req, &#34;The specified year is not a valid year.&#34;)

                        elif key == &#39;maxentries&#39;:
                            maxentries_pattern = re.compile(MAX_ENTRIES_PATTERN)
                            if re.match(maxentries_pattern, cut[1]):
                                d[key].append(cut[1])
                            else:
                                raise RequestSyntaxError(req, &#34;The number of maximum entries you specified is not valid.&#34;)

                        elif key == &#39;id&#39;:
                            id_nb_pattern = re.compile(IMDB_ID_NB_PATTERN)
                            id_tt_pattern = re.compile(IMDB_ID_TT_PATTERN)

                            if re.match(id_nb_pattern, cut[1]):
                                d[key].append(cut[1])
                            elif re.match(id_tt_pattern, cut[1]):
                                d[key].append(cut[1][2:])  #  removes the tt
                            else:
                                raise RequestSyntaxError(req, &#34;The entered IMDb ID is not valid.&#34;)

                        else:
                            d[key].append(cut[1])

                if len(cut) &gt; 2:  # if there is more that one &#39;:&#39;
                    raise RequestSyntaxError(req, &#34;You have put too many &#39;&#34; + KEY_SEPARATOR + &#34;&#39;.&#34;)

                if len(cut) == 1:  # that means no &#39;:&#39; was found
                    if len(u_req) != 0:  # useful when there is double spaces or &#34;, &#34;
                        if u_req[0] == HASHTAG_KEY:  # twitter hashtags
                            if len(u_req) == 1:
                                raise RequestSyntaxError(req, &#34;Single &#39;&#34; + HASHTAG_KEY + &#34;&#39; detected.&#34;)
                            d[&#39;hashtag&#39;].append(u_req[1:])
                            if not d[&#39;src&#39;]:  # if there is no source specified, then it becomes twitter
                                d[&#39;src&#39;].append(&#39;twitter&#39;)

                        elif u_req[0] == USER_KEY:  # twitter users
                            if len(u_req) == 1:
                                raise RequestSyntaxError(req, &#34;Single &#39;&#34; + USER_KEY + &#34;&#39; detected.&#34;)
                            d[&#39;user&#39;].append(u_req[1:])
                            if not d[&#39;src&#39;]:
                                d[&#39;src&#39;].append(&#39;twitter&#39;)

                        elif u_req[0] == TMP_FILM_OPERATOR:  # imdb film title
                            d[&#39;film&#39;].append(film_titles[films_index])
                            films_index += 1
                            if not d[&#39;src&#39;]:
                                d[&#39;src&#39;].append(&#39;imdb&#39;)

                        else:  # if no special char was found, then it&#39;s a &#34;regular&#34; keyword
                            d[&#39;search&#39;].append(u_req)

            tmp.append(d)  # we put the constructed dict in the interpreted_request attr
        self.interpreted_request = tmp

    def __post_process_request(self):
        for idx, d in enumerate(self.interpreted_request):
            self.interpreted_request[idx] = {k: list(set(j)) for k, j in d.items()}  # removes duplicates
            for k in SEARCH_KEYS:
                if k not in d.keys():
                    self.interpreted_request[idx][k] = []  # gives an empty list if the key is not found

            for mk in METADATA_KEYS:
                self.interpreted_request[idx][mk] = []

    def __validate_semantic(self):
        &#39;&#39;&#39;Method to validate the coherence of a request.
        For example, a request that has a correct syntax but that contains both &#39;hashtag&#39; and &#39;film&#39; is not coherent,
        as we will only search for film titles in IMDb (and IMDb have any paradigm that resembles hashtags).
        As the prerequisites for the various source are very different, specific methods are called to ensure their
        validity : __validate_semantic_imdb, __validate_semantic_twitter and __validate_semantic_local
        &#39;&#39;&#39;
        # print(self.__repr__())
        for request in self.interpreted_request:

            # == Multiple sources
            if len(request[&#39;src&#39;]) &gt; 1:
                raise RequestSyntaxError(self.str_request, &#34;You cannot specify more than one source for your data extraction &#34; +
                                         &#34; in a single request.&#34;)
            elif not request[&#39;src&#39;]:
                raise RequestSyntaxError(self.str_request, &#34;You haven&#39;t specified a source for your data.&#34;)

            # == IMDb
            if &#39;imdb&#39; in request[&#39;src&#39;]:
                self.__validate_semantics_imdb(request)
            else:
                if request[&#39;film&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;Films are specific to IMDb searches.&#34;)
                if request[&#39;year&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;You can specify a year only when the source is IMDb &#34; +
                                             &#34;and you are already searching for a particular movie.&#34;)
                if request[&#39;id&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;IDs are specific to IMDb searches.&#34;)

            # == Twitter
            if &#39;twitter&#39; in request[&#39;src&#39;]:
                self.__validate_semantics_twitter(request)
            else:
                if request[&#39;hashtag&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;Hashtags are specific to Twitter searches.&#34;)
                if request[&#39;user&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;Users are specific to Twitter searches.&#34;)

            # == Local data
            if &#39;local&#39; in request[&#39;src&#39;]:
                self.__validate_semantics_local(request)
            else:
                if request[&#39;colname&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;You can specify the column to search in only for local searches.&#34;)
                if request[&#39;type&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;You can only specify the type of data for requests on imported data.&#34;)

            # == Languages
            if len(request[&#39;lang&#39;]) &lt; 1:
                request[&#39;lang&#39;] = [DEFAULT_LANG]

            if request[&#39;target&#39;]:
                for idx, lang in enumerate(request[&#39;target&#39;]):
                    output = is_valid_target_lang(lang)
                    if output:
                        request[&#39;target&#39;][idx] = output  # conversion to the ISO norm
                    else:
                        raise RequestSyntaxError(self.str_request, &#34;\&#34;&#34; + lang + &#34;\&#34; is not a supported target language&#34;)

            if request[&#39;target&#39;] and not is_api_up(is_googletrans_api_up, TIMEOUT_DURATION, &#34;gooletrans&#34;):
                if not is_connected_internet():
                    raise RequestSyntaxError(self.str_request, &#34;You are not connected to the Internet at the moment. &#34; +
                                             &#34;No translation can be provided.&#34;)
                raise RequestSyntaxError(self.str_request, &#34;You don&#39;t have any Google Translate API credits left for &#34; +
                                         &#34;today or the API is not available. No translation can be provided at the moment.&#34;)

            # == Max entries
            if not request[&#39;maxentries&#39;]:
                request[&#39;maxentries&#39;] = [DEFAULT_MAX_ENTRIES]

    def __validate_semantics_local(self, request):
        &#39;&#39;&#39;Method that raises a RequestSyntaxError if the semantic prerequisites are not met for a local search.

        request -- a dict that contains a request are described in this module
        &#39;&#39;&#39;
        if not self.__check_presence_necessary_keys(request, [&#39;colname&#39;, &#39;search&#39;]):
            raise RequestSyntaxError(self.str_request, &#34;You must at least specify a column with the &#39;colname&#39; keyword, or a search key.&#34;)

        if self.local_data is None:
            raise RequestSyntaxError(self.str_request, &#34;In order to request an analysis on your imported data,&#34; +
                                     &#34; you have to import your data first.&#34;)

        if len(request[&#39;colname&#39;]) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;You can only search for data in atmost one column.&#34;)
        elif len(request[&#39;colname&#39;]) == 1:
            try:
                self.local_data.data[request[&#39;colname&#39;][0]]
            except KeyError:
                raise RequestSyntaxError(self.str_request, &#34;The specified column name doesn&#39;t exist&#34;)

        if len(request[&#39;type&#39;]) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;You can only specify one type of data.&#34;)
        elif len(request[&#39;type&#39;]) == 1 and (request[&#39;type&#39;][0] != &#39;tweets&#39; and request[&#39;type&#39;][0] != &#39;reviews&#39;):
            raise RequestSyntaxError(self.str_request, &#34;The specified data type can only be either &#39;tweets&#39; or &#39;reviews&#39;&#34;)
        elif not request[&#39;type&#39;]:
            request[&#39;type&#39;] = &#39;tweets&#39;  # if no type is specified, then we give it one

        if len(request[&#39;lang&#39;]) &gt; 1 and not is_api_up(is_detectlanguage_api_up, TIMEOUT_DURATION, &#34;detectlanguage&#34;):
            message = (&#34;No language detection can be provided at the moment, &#34; +
                       &#34;you have to specify the language in which the data is.&#34;)
            if not is_connected_internet():
                raise RequestSyntaxError(self.str_request, &#34;You are not connected to the Internet. &#34; +
                                         message)
            raise RequestSyntaxError(self.str_request, &#34;You don&#39;t have any detect-language API credits left for &#34; +
                                     &#34;today or the API is not available. &#34; + message)

    def __validate_semantics_imdb(self, request):
        &#39;&#39;&#39;Method that raises a RequestSyntaxError if the semantic prerequisites are not met for a IMDb search.

        request -- a dict that contains a request are described in this module
        &#39;&#39;&#39;
        if not is_connected_internet():
            if self.imdb_webdriver:
                raise RequestSyntaxError(self.str_request, &#34;You are not connected to the Internet, please come back &#34; +
                                         &#34; when you have a stable Internet connection.&#34;)
            else:
                raise RequestSyntaxError(self.str_request, &#34;No webdriver could be loaded and you are not connected &#34; +
                                         &#34;to the Internet. Please get a stable connection and restart the program.&#34;)

        if not self.imdb_webdriver:
            raise RequestSyntaxError(self.str_request, &#34;No webdriver could be detected on your machine. &#34; +
                                     &#34;If neither a recent Google Chrome version nor a Firefox version are installed on your machine, &#34; +
                                     &#34;please do so. Otherwise, wait a few seconds or restart the program.&#34;)

        if not self.__check_presence_necessary_keys(request, [&#39;film&#39;, &#39;id&#39;]):
            raise RequestSyntaxError(self.str_request, &#34;In order to search for a film, you must specify an ID &#34; +
                                     &#34;or a film name between quotation marks.&#34;)

        if len(request[&#39;lang&#39;]) &gt; 0 and &#39;en&#39; not in request[&#39;lang&#39;]:
            raise RequestSyntaxError(self.str_request, &#34;IMDb can only be searched in english as there is only &#34; +
                                     &#34;english data on this website.&#34;)

        if len(request[&#39;film&#39;]) &gt; 1 or len(request[&#39;id&#39;]) &gt; 1 or len(request[&#39;year&#39;]) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;You can&#39;t search for more that one film at the moment.&#34;)

    def __validate_semantics_twitter(self, request):
        &#39;&#39;&#39;Method that raises a RequestSyntaxError if the semantic prerequisites are not met for a Twitter search.

        request -- a dict that contains a request are described in this module
        &#39;&#39;&#39;
        if not is_connected_internet():
            raise RequestSyntaxError(self.str_request, &#34;You are not connected to the Internet, please come back &#34; +
                                     &#34; when you have a stable Internet connection.&#34;)

        if not is_api_up(is_tweepy_up, &#34;twitter&#34;):
            if not is_tweepy_api_key_valid():
                raise RequestSyntaxError(self.str_request, &#34;Your Twitter API key is not valid.&#34;)
            raise RequestSyntaxError(self.str_request, &#34;The Twitter API is not available at the moment, please &#34; +
                                     &#34;come back later.&#34;)

        if not self.__check_presence_necessary_keys(request, [&#39;hashtag&#39;, &#39;user&#39;]):
            raise RequestSyntaxError(self.str_request, &#34;In order to search for Twitter data, you must at least &#34; +
                                     &#34;specify a hashtag with a leading &#39;&#34; + HASHTAG_KEY + &#34;&#39; key or a user with a &#34; +
                                     &#34;leading &#39;&#34; +
                                     USER_KEY + &#34;&#39; key.&#34;)

        if len(request[&#39;user&#39;]) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;You can&#39;t search for more that one user at the moment.&#34;)

    def __check_presence_necessary_keys(self, request: dict, keys: list):
        for key in keys:  # one if these keys has to be in the request
            if request[key]:
                return True
        return False

    def handle_request(self, req):
        &#39;&#39;&#39;This method is used by the Controller to send requests to the model.
        If the request is valid, it is processed by the various DataExtractors and the output data is then analyzed
        and classified. Otherwise, different errors are thrown to the Controller.

        req -- str that has to comply to the specified syntax and semantics of the program
        &#39;&#39;&#39;
        # checks syntax validity
        self.__interpret_request(req)
        extracted_data = pd.DataFrame()  # list where the data returned to the Controller will be stored

        self.processed_data = []  # will be the list of structured data analyzed and classified for each sub-request

        for d in self.interpreted_request:  # depending on the request src, we send the data to the various extractors
            print(self.__repr__())

            if &#39;twitter&#39; in d[&#39;src&#39;]:
                try:
                    extracted_data = requetes_twitter(d)
                except ValueError as e:
                    raise RequestSyntaxError(self.str_request, str(e))
                except Exception:
                    traceback.print_exc()
                    raise RequestSyntaxError(self.str_request, &#34;An unexpected error occurred with your Twitter search.&#34;)

            elif &#39;imdb&#39; in d[&#39;src&#39;]:
                try:
                    extracted_data = requetes_imdb(d, self.imdb_webdriver)
                except ValueError as ve:  # in the case the ID doesn&#39;t exist
                    raise RequestSyntaxError(self.str_request, str(ve))
                except Exception as e:
                    # catches the exceptions thrown by the imdb extractor. At least two are needed
                    #    - when the user needs to specify a year
                    #    - when the user needs to specify an imdb id because specifying a year wasn&#39;t enough
                    traceback.print_exc()
                    raise RequestSyntaxError(self.str_request, &#34;An unexpected error occurred with your IMDb search.&#34;)

            elif &#39;local&#39; in d[&#39;src&#39;]:
                try:
                    extracted_data = self.local_data.extract_data(d)  # extract rows from the loaded dataset
                except ValueError as e:  #  in the case no row was found
                    raise RequestSyntaxError(self.str_request, e)

            print(&#34;Result from &#34; + d[&#39;src&#39;][0] + &#34; extraction:\n&#34; + str(extracted_data.head()))

            if &#39;src_lang&#39; in extracted_data:  # if we were able to retrieve detect languages
                per_lang_extracted_data = []
                for lang in d[&#39;lang&#39;]:
                    per_lang_data = extracted_data.loc[extracted_data[&#39;src_lang&#39;] == lang]  # divides the dataset by lang
                    if not per_lang_data.empty:
                        if &#39;type&#39; in d:
                            if &#39;tweets&#39; in d[&#39;type&#39;]:  # in the case those are tweets
                                polarity = self.__get_pol_twitter(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                            else:  # or reviews
                                polarity = self.__get_pol_imdb(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                        elif &#39;twitter&#39; in d[&#39;src&#39;]:
                            polarity = self.__get_pol_twitter(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                        elif &#39;imdb&#39; in d[&#39;src&#39;]:
                            polarity = self.__get_pol_imdb(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                        per_lang_data[POL_COLUMN_NAME] = np.asarray(polarity)  # add the polarity to the df
                        per_lang_extracted_data.append(per_lang_data)

                extracted_data = pd.concat(per_lang_extracted_data)  # .sort_index() (maybe it will be sorted w/o it?)

            else:  # otherwise, we use the first language, hoping it will cover most of the data
                if &#39;type&#39; in d:
                    if &#39;tweets&#39; in d[&#39;type&#39;]:  # in the case those are tweets
                        polarity = self.__get_pol_twitter(extracted_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])
                    else:  # or reviews
                        polarity = self.__get_pol_imdb(extracted_data, d[&#39;lang&#39;][0], d[&#39;colname&#39;][0])
                    extracted_data[POL_COLUMN_NAME] = np.asarray(polarity)  # add the polarity to the df
                elif &#39;twitter&#39; in d[&#39;src&#39;]:
                    polarity = self.__get_pol_twitter(per_lang_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])
                elif &#39;imdb&#39; in d[&#39;src&#39;]:
                    polarity = self.__get_pol_imdb(per_lang_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])

            for lang in d[&#39;target&#39;]:
                extracted_data = translate_df(extracted_data, d, lang)

            # we extract the frequency of the most common words and had it to the output
            request_output = RequestOutput(self.str_request, d, extracted_data, POL_COLUMN_NAME)
            self.processed_data.append(request_output)

        print(self.__repr__())
        return self.processed_data

    def import_local_data(self, path):
        self.local_data = LocalImporter(path)
        try:
            print(self.local_data.data.head())
        except AttributeError:
            print(&#34;The file could not be read hence the imported data is None.&#34;)

    def __get_pol_twitter(self, extracted_data, language, colname):
        # wrapper method to lighten the code for language choice
        try:  # in the event no model was loaded
            if language == &#39;fr&#39;:
                return self.vc_fr_twitter.classify(extracted_data, colname, pos_max_thresh=0.7, neg_max_thresh=0.2)
            if language == &#39;en&#39;:
                return self.vc_en_twitter.classify(extracted_data, colname, pos_max_thresh=0.7, neg_max_thresh=0.2)
        except AttributeError as ae:
            raise RequestSyntaxError(self.str_request, str(ae))

    def __get_pol_imdb(self, extracted_data, language, colname):
        # wrapper method to lighten the code for language choice
        try:  # in the event no model was loaded
            if language == &#39;fr&#39;:
                return self.vc_fr_imdb.classify(extracted_data, colname, pos_max_thresh=0.7, neg_max_thresh=0.2)
            if language == &#39;en&#39;:
                return self.vc_en_imdb.classify(extracted_data, colname, pos_max_thresh=0.7, neg_max_thresh=0.2)
        except AttributeError as ae:
            raise RequestSyntaxError(self.str_request, str(ae))


class RequestSyntaxError(Exception):
    &#39;&#39;&#39;Base class for request syntax exceptions

    Attributes:
        req -- input request which caused the error
        message -- explanation of the error
    &#39;&#39;&#39;

    def __init__(self, req, message):
        self.req = req
        self.message = message
        super().__init__(self.message)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.model.requests_handler.RequestSyntaxError"><code class="flex name class">
<span>class <span class="ident">RequestSyntaxError</span></span>
<span>(</span><span>req, message)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for request syntax exceptions</p>
<h2 id="attributes">Attributes</h2>
<p>req &ndash; input request which caused the error
message &ndash; explanation of the error</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RequestSyntaxError(Exception):
    &#39;&#39;&#39;Base class for request syntax exceptions

    Attributes:
        req -- input request which caused the error
        message -- explanation of the error
    &#39;&#39;&#39;

    def __init__(self, req, message):
        self.req = req
        self.message = message
        super().__init__(self.message)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="src.model.requests_handler.RequestsHandler"><code class="flex name class">
<span>class <span class="ident">RequestsHandler</span></span>
</code></dt>
<dd>
<div class="desc"><p>Main class of the package. This is where the requests from the control will be interpreted. Every flow of data
will go through this class to go in or out of the model package.</p>
<p>Constructor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RequestsHandler:
    &#39;&#39;&#39;
    Main class of the package. This is where the requests from the control will be interpreted. Every flow of data
    will go through this class to go in or out of the model package.
    &#39;&#39;&#39;

    def __init__(self):
        &#39;&#39;&#39;
        Constructor
        &#39;&#39;&#39;

        self.str_request = &#34;&#34;  # the unprocessed request
        self.processed_data = None  # the data to return to the controller
        # self.api_keys = None #might not be useful as an attribute
        self.interpreted_request = {}  # will contain the list of dict extracted from the original str

        # Data extractors
        # self.imdb = new instance of extractor, with the argument read_api_key(&#39;imdb&#39;)
        # self.twitter = new instance of extractor, with the argument read_api_key(&#39;twitter&#39;)
        self.local_data = None  # new instance of local extractor
        self.imdb_webdriver = None

        # Classifiers
        # self.vc_en_imdb = get_imdb_long_reviews_vc()  # instantiates the VoteClassifier for IMDb
        self.vc_en_twitter = VoteClassifier(&#34;Twitter (en)&#34;)
        self.vc_fr_twitter = VoteClassifier(&#34;Twitter (fr)&#34;)
        self.vc_en_imdb = VoteClassifier(&#34;IMDb (en)&#34;)
        self.vc_fr_imdb = VoteClassifier(&#34;IMDb (fr)&#34;)

    def load_classifiers(self, multithreading=True):
        time.sleep(0.1)  # this line allows the time for the GUI to load correctly
        if multithreading:  # with multithreading
            vcs = [(self.vc_en_twitter, get_twitter_en_vc),
                   (self.vc_fr_twitter, get_twitter_fr_vc),
                   (self.vc_en_imdb, get_imdb_short_reviews_vc),
                   (self.vc_fr_imdb, get_imdb_allocine_vc)]

            threads = []

            for vc, get in vcs:
                t = Thread(target=get, args=[vc])
                t.start()
                threads.append(t)

            for t in threads:
                t.join()

        else:  # without multithreading
            get_twitter_en_vc(self.vc_en_twitter)
            get_imdb_short_reviews_vc(self.vc_en_imdb)
            get_twitter_fr_vc(self.vc_fr_twitter)

    def __repr__(self):
        s = &#34;\nRequestsHandler state:\n&#34;
        s += &#34;\t* Raw request: \&#34;&#34; + self.str_request + &#34;\&#34;\n&#34;
        s += &#34;\t* Interpreted request: &#34; + str(self.interpreted_request) + &#34;\n&#34;
        s += &#34;\t* Imported data? &#34; + (&#34;No&#34; if self.local_data is None else &#34;Yes&#34;) + &#34;.\n&#34;
        s += &#34;\t* Has the data been processed? &#34; + (&#34;No&#34; if self.processed_data is None else &#34;Yes:\n&#34; +
                                                    str(self.processed_data)) + &#34;.\n&#34;
        s += (&#34;\t* Webdriver: &#34; + (&#34;Firefox&#34; if isinstance(self.imdb_webdriver, firefox.webdriver.WebDriver) else &#34;&#34;) +
              (&#34;Chrome&#34; if isinstance(self.imdb_webdriver, chrome.webdriver.WebDriver) else &#34;&#34;) +
              (&#34;None&#34; if not self.imdb_webdriver else &#34;&#34;)) + &#34;.\n\n&#34;
        return s

    def __interpret_request(self, request):
        if self.__check_basic_requirements(request):  # raises exceptions for basic invalidity
            self.str_request = request  # if there is no obvious errors, then it is passed as a value of the attribute
        self.__parse_request()
        self.__post_process_request()

        # checks semantic validity
        self.__validate_semantic()
        return True

    def __check_basic_requirements(self, req):
        &#39;&#39;&#39;Checks if the parameter of the method meets the basic requirements to be a valid request.
        The request is a str, it is not empty and contains only valid characters.
        &#39;&#39;&#39;

        if not(isinstance(req, str)):  # is a str
            raise RequestSyntaxError(req, &#34;The request is expected to be a str.&#34;)

        if len(req) &lt; 1:  # is not empty
            raise RequestSyntaxError(req, &#34;The request is empty.&#34;)

        if not(self.__match_regex(req)):  # contains the expected characters
            raise RequestSyntaxError(req, &#34;The request doesn&#39;t meet the expected syntax (invalid characters).&#34; +
                                     &#34;Please consult our User Manual.&#34;)

        return True

    def __match_regex(self, req):
        &#39;&#39;&#39;Method using a regular expression to check if the syntax of the request matches the requirements expected.
        &#39;&#39;&#39;

        basic_expression = ALLOWED_CHARS + HASHTAG_KEY + USER_KEY + KEY_SEPARATOR + INTERSECTION_SEPARATOR + UNION_SEPARATOR
        film_title_expression = ALLOWED_CHARS + FILM_TITLE_ALLOWED_CHARS  # film titles are treated separately

        pattern = r&#39;([{0}]+|&#34;[{1}]+&#34;+)&#39;.format(basic_expression, film_title_expression)

        regex = re.compile(pattern)
        results = regex.findall(req)  # finds every matching substring, treats separately film titles and the rest
        # print(&#34;result=&#34; + str(result))

        if len(results) &lt; 1:
            raise RequestSyntaxError(req, &#34;The request is empty.&#34;)

        length_sum = 0
        for result in results:
            for r in result:
                length_sum += len(r)

        # if the sum of the lengths is equal to what findall has found, then it matched
        # otherwise, there is an error
        return length_sum == len(req)

    def __find_films_and_replace(self):
        &#39;&#39;&#39;Films titles are tricky to deal with in this project since they can use a lot of different characters.
        We have to treat them apart from the rest of the str.
        &#39;&#39;&#39;
        film_titles = []
        if FILM_KEY not in self.str_request:  # if no film title is detected in the original request
            return film_titles, self.str_request  # then nothing is done

        # we detect the film titles with their by looking at quotations marks.
        pattern = r&#39;&#34;[{}]+&#34;&#39;.format(ALLOWED_CHARS + FILM_TITLE_ALLOWED_CHARS)
        regex = re.compile(pattern)
        film_titles = regex.findall(self.str_request)

        tmp_request = self.str_request

        for i in range((len(film_titles))):
            # since titles can hold a broader range of characters, we have to replace them with a special key
            tmp_request = tmp_request.replace(film_titles[i], TMP_FILM_OPERATOR)
            film_titles[i] = film_titles[i].replace(FILM_KEY, &#39;&#39;)  # removes the quotation mark from the stored value

        return (film_titles, tmp_request)

    def __parse_request(self):
        &#39;&#39;&#39;Method that naively goes through the raw string to parse it into a dict.
        Does some basic filtering in order to raise errors if needed.
        &#39;&#39;&#39;

        film_titles, tmp_request = self.__find_films_and_replace()  # removes film titles to deal with tricky characters
        tmp_request = tmp_request.lower()  # only the tmp_request is turned to lowercase
        films_index = 0  # index to keep count of where we are in the film&#39;s list
        intersect_requests = tmp_request.split(INTERSECTION_SEPARATOR)  # splits into several sub-requests

        if len(intersect_requests) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;Multiple requests are not supported at the time.&#34; +
                                     &#34; It will be supported in a future update.&#34;)

        tmp = []
        # by using a temporary variable, we ensure that the interpreted_request will only be updated if
        # the request is fully valid

        for req in intersect_requests:
            d = defaultdict(list)  # new empty dict of lists for this request

            if req == len(req) * UNION_SEPARATOR:  # when the request only contains spaces
                raise RequestSyntaxError(req, &#34;One of the subrequest is only made of &#39;&#34; + UNION_SEPARATOR + &#34;&#39;.&#34;)

            if len(req) == 0:  # or if the sequence &#39;,,&#39; is met
                raise RequestSyntaxError(req, &#34;One of the subrequest is empty.&#34;)

            union_requests = req.split(UNION_SEPARATOR)  # splits into keywords to interpret

            for u_req in union_requests:

                cut = u_req.split(KEY_SEPARATOR)

                if len(cut) == 2:
                    key = None
                    for search_key in SEARCH_KEYS:  # we verify if the key specified is a known key
                        if cut[0] == search_key:
                            key = search_key

                    if key is None or not cut[1]:  # if the word preceding the KEY_SEPARATOR isn&#39;t in SEARCH_KEYS then :
                        raise RequestSyntaxError(req, &#34;The request contains invalid filters.&#34;)
                    else:
                        if key == &#39;lang&#39;:
                            try:
                                cut = convert_lang_format(cut[1])
                            except ValueError as e:
                                raise RequestSyntaxError(req, e)
                            d[key].append(cut)

                        elif key == &#39;year&#39;:
                            year_pattern = re.compile(FILM_YEAR_PATTERN)
                            if re.match(year_pattern, cut[1]):
                                d[key].append(cut[1])
                            else:
                                raise RequestSyntaxError(req, &#34;The specified year is not a valid year.&#34;)

                        elif key == &#39;maxentries&#39;:
                            maxentries_pattern = re.compile(MAX_ENTRIES_PATTERN)
                            if re.match(maxentries_pattern, cut[1]):
                                d[key].append(cut[1])
                            else:
                                raise RequestSyntaxError(req, &#34;The number of maximum entries you specified is not valid.&#34;)

                        elif key == &#39;id&#39;:
                            id_nb_pattern = re.compile(IMDB_ID_NB_PATTERN)
                            id_tt_pattern = re.compile(IMDB_ID_TT_PATTERN)

                            if re.match(id_nb_pattern, cut[1]):
                                d[key].append(cut[1])
                            elif re.match(id_tt_pattern, cut[1]):
                                d[key].append(cut[1][2:])  #  removes the tt
                            else:
                                raise RequestSyntaxError(req, &#34;The entered IMDb ID is not valid.&#34;)

                        else:
                            d[key].append(cut[1])

                if len(cut) &gt; 2:  # if there is more that one &#39;:&#39;
                    raise RequestSyntaxError(req, &#34;You have put too many &#39;&#34; + KEY_SEPARATOR + &#34;&#39;.&#34;)

                if len(cut) == 1:  # that means no &#39;:&#39; was found
                    if len(u_req) != 0:  # useful when there is double spaces or &#34;, &#34;
                        if u_req[0] == HASHTAG_KEY:  # twitter hashtags
                            if len(u_req) == 1:
                                raise RequestSyntaxError(req, &#34;Single &#39;&#34; + HASHTAG_KEY + &#34;&#39; detected.&#34;)
                            d[&#39;hashtag&#39;].append(u_req[1:])
                            if not d[&#39;src&#39;]:  # if there is no source specified, then it becomes twitter
                                d[&#39;src&#39;].append(&#39;twitter&#39;)

                        elif u_req[0] == USER_KEY:  # twitter users
                            if len(u_req) == 1:
                                raise RequestSyntaxError(req, &#34;Single &#39;&#34; + USER_KEY + &#34;&#39; detected.&#34;)
                            d[&#39;user&#39;].append(u_req[1:])
                            if not d[&#39;src&#39;]:
                                d[&#39;src&#39;].append(&#39;twitter&#39;)

                        elif u_req[0] == TMP_FILM_OPERATOR:  # imdb film title
                            d[&#39;film&#39;].append(film_titles[films_index])
                            films_index += 1
                            if not d[&#39;src&#39;]:
                                d[&#39;src&#39;].append(&#39;imdb&#39;)

                        else:  # if no special char was found, then it&#39;s a &#34;regular&#34; keyword
                            d[&#39;search&#39;].append(u_req)

            tmp.append(d)  # we put the constructed dict in the interpreted_request attr
        self.interpreted_request = tmp

    def __post_process_request(self):
        for idx, d in enumerate(self.interpreted_request):
            self.interpreted_request[idx] = {k: list(set(j)) for k, j in d.items()}  # removes duplicates
            for k in SEARCH_KEYS:
                if k not in d.keys():
                    self.interpreted_request[idx][k] = []  # gives an empty list if the key is not found

            for mk in METADATA_KEYS:
                self.interpreted_request[idx][mk] = []

    def __validate_semantic(self):
        &#39;&#39;&#39;Method to validate the coherence of a request.
        For example, a request that has a correct syntax but that contains both &#39;hashtag&#39; and &#39;film&#39; is not coherent,
        as we will only search for film titles in IMDb (and IMDb have any paradigm that resembles hashtags).
        As the prerequisites for the various source are very different, specific methods are called to ensure their
        validity : __validate_semantic_imdb, __validate_semantic_twitter and __validate_semantic_local
        &#39;&#39;&#39;
        # print(self.__repr__())
        for request in self.interpreted_request:

            # == Multiple sources
            if len(request[&#39;src&#39;]) &gt; 1:
                raise RequestSyntaxError(self.str_request, &#34;You cannot specify more than one source for your data extraction &#34; +
                                         &#34; in a single request.&#34;)
            elif not request[&#39;src&#39;]:
                raise RequestSyntaxError(self.str_request, &#34;You haven&#39;t specified a source for your data.&#34;)

            # == IMDb
            if &#39;imdb&#39; in request[&#39;src&#39;]:
                self.__validate_semantics_imdb(request)
            else:
                if request[&#39;film&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;Films are specific to IMDb searches.&#34;)
                if request[&#39;year&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;You can specify a year only when the source is IMDb &#34; +
                                             &#34;and you are already searching for a particular movie.&#34;)
                if request[&#39;id&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;IDs are specific to IMDb searches.&#34;)

            # == Twitter
            if &#39;twitter&#39; in request[&#39;src&#39;]:
                self.__validate_semantics_twitter(request)
            else:
                if request[&#39;hashtag&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;Hashtags are specific to Twitter searches.&#34;)
                if request[&#39;user&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;Users are specific to Twitter searches.&#34;)

            # == Local data
            if &#39;local&#39; in request[&#39;src&#39;]:
                self.__validate_semantics_local(request)
            else:
                if request[&#39;colname&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;You can specify the column to search in only for local searches.&#34;)
                if request[&#39;type&#39;]:
                    raise RequestSyntaxError(self.str_request, &#34;You can only specify the type of data for requests on imported data.&#34;)

            # == Languages
            if len(request[&#39;lang&#39;]) &lt; 1:
                request[&#39;lang&#39;] = [DEFAULT_LANG]

            if request[&#39;target&#39;]:
                for idx, lang in enumerate(request[&#39;target&#39;]):
                    output = is_valid_target_lang(lang)
                    if output:
                        request[&#39;target&#39;][idx] = output  # conversion to the ISO norm
                    else:
                        raise RequestSyntaxError(self.str_request, &#34;\&#34;&#34; + lang + &#34;\&#34; is not a supported target language&#34;)

            if request[&#39;target&#39;] and not is_api_up(is_googletrans_api_up, TIMEOUT_DURATION, &#34;gooletrans&#34;):
                if not is_connected_internet():
                    raise RequestSyntaxError(self.str_request, &#34;You are not connected to the Internet at the moment. &#34; +
                                             &#34;No translation can be provided.&#34;)
                raise RequestSyntaxError(self.str_request, &#34;You don&#39;t have any Google Translate API credits left for &#34; +
                                         &#34;today or the API is not available. No translation can be provided at the moment.&#34;)

            # == Max entries
            if not request[&#39;maxentries&#39;]:
                request[&#39;maxentries&#39;] = [DEFAULT_MAX_ENTRIES]

    def __validate_semantics_local(self, request):
        &#39;&#39;&#39;Method that raises a RequestSyntaxError if the semantic prerequisites are not met for a local search.

        request -- a dict that contains a request are described in this module
        &#39;&#39;&#39;
        if not self.__check_presence_necessary_keys(request, [&#39;colname&#39;, &#39;search&#39;]):
            raise RequestSyntaxError(self.str_request, &#34;You must at least specify a column with the &#39;colname&#39; keyword, or a search key.&#34;)

        if self.local_data is None:
            raise RequestSyntaxError(self.str_request, &#34;In order to request an analysis on your imported data,&#34; +
                                     &#34; you have to import your data first.&#34;)

        if len(request[&#39;colname&#39;]) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;You can only search for data in atmost one column.&#34;)
        elif len(request[&#39;colname&#39;]) == 1:
            try:
                self.local_data.data[request[&#39;colname&#39;][0]]
            except KeyError:
                raise RequestSyntaxError(self.str_request, &#34;The specified column name doesn&#39;t exist&#34;)

        if len(request[&#39;type&#39;]) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;You can only specify one type of data.&#34;)
        elif len(request[&#39;type&#39;]) == 1 and (request[&#39;type&#39;][0] != &#39;tweets&#39; and request[&#39;type&#39;][0] != &#39;reviews&#39;):
            raise RequestSyntaxError(self.str_request, &#34;The specified data type can only be either &#39;tweets&#39; or &#39;reviews&#39;&#34;)
        elif not request[&#39;type&#39;]:
            request[&#39;type&#39;] = &#39;tweets&#39;  # if no type is specified, then we give it one

        if len(request[&#39;lang&#39;]) &gt; 1 and not is_api_up(is_detectlanguage_api_up, TIMEOUT_DURATION, &#34;detectlanguage&#34;):
            message = (&#34;No language detection can be provided at the moment, &#34; +
                       &#34;you have to specify the language in which the data is.&#34;)
            if not is_connected_internet():
                raise RequestSyntaxError(self.str_request, &#34;You are not connected to the Internet. &#34; +
                                         message)
            raise RequestSyntaxError(self.str_request, &#34;You don&#39;t have any detect-language API credits left for &#34; +
                                     &#34;today or the API is not available. &#34; + message)

    def __validate_semantics_imdb(self, request):
        &#39;&#39;&#39;Method that raises a RequestSyntaxError if the semantic prerequisites are not met for a IMDb search.

        request -- a dict that contains a request are described in this module
        &#39;&#39;&#39;
        if not is_connected_internet():
            if self.imdb_webdriver:
                raise RequestSyntaxError(self.str_request, &#34;You are not connected to the Internet, please come back &#34; +
                                         &#34; when you have a stable Internet connection.&#34;)
            else:
                raise RequestSyntaxError(self.str_request, &#34;No webdriver could be loaded and you are not connected &#34; +
                                         &#34;to the Internet. Please get a stable connection and restart the program.&#34;)

        if not self.imdb_webdriver:
            raise RequestSyntaxError(self.str_request, &#34;No webdriver could be detected on your machine. &#34; +
                                     &#34;If neither a recent Google Chrome version nor a Firefox version are installed on your machine, &#34; +
                                     &#34;please do so. Otherwise, wait a few seconds or restart the program.&#34;)

        if not self.__check_presence_necessary_keys(request, [&#39;film&#39;, &#39;id&#39;]):
            raise RequestSyntaxError(self.str_request, &#34;In order to search for a film, you must specify an ID &#34; +
                                     &#34;or a film name between quotation marks.&#34;)

        if len(request[&#39;lang&#39;]) &gt; 0 and &#39;en&#39; not in request[&#39;lang&#39;]:
            raise RequestSyntaxError(self.str_request, &#34;IMDb can only be searched in english as there is only &#34; +
                                     &#34;english data on this website.&#34;)

        if len(request[&#39;film&#39;]) &gt; 1 or len(request[&#39;id&#39;]) &gt; 1 or len(request[&#39;year&#39;]) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;You can&#39;t search for more that one film at the moment.&#34;)

    def __validate_semantics_twitter(self, request):
        &#39;&#39;&#39;Method that raises a RequestSyntaxError if the semantic prerequisites are not met for a Twitter search.

        request -- a dict that contains a request are described in this module
        &#39;&#39;&#39;
        if not is_connected_internet():
            raise RequestSyntaxError(self.str_request, &#34;You are not connected to the Internet, please come back &#34; +
                                     &#34; when you have a stable Internet connection.&#34;)

        if not is_api_up(is_tweepy_up, &#34;twitter&#34;):
            if not is_tweepy_api_key_valid():
                raise RequestSyntaxError(self.str_request, &#34;Your Twitter API key is not valid.&#34;)
            raise RequestSyntaxError(self.str_request, &#34;The Twitter API is not available at the moment, please &#34; +
                                     &#34;come back later.&#34;)

        if not self.__check_presence_necessary_keys(request, [&#39;hashtag&#39;, &#39;user&#39;]):
            raise RequestSyntaxError(self.str_request, &#34;In order to search for Twitter data, you must at least &#34; +
                                     &#34;specify a hashtag with a leading &#39;&#34; + HASHTAG_KEY + &#34;&#39; key or a user with a &#34; +
                                     &#34;leading &#39;&#34; +
                                     USER_KEY + &#34;&#39; key.&#34;)

        if len(request[&#39;user&#39;]) &gt; 1:
            raise RequestSyntaxError(self.str_request, &#34;You can&#39;t search for more that one user at the moment.&#34;)

    def __check_presence_necessary_keys(self, request: dict, keys: list):
        for key in keys:  # one if these keys has to be in the request
            if request[key]:
                return True
        return False

    def handle_request(self, req):
        &#39;&#39;&#39;This method is used by the Controller to send requests to the model.
        If the request is valid, it is processed by the various DataExtractors and the output data is then analyzed
        and classified. Otherwise, different errors are thrown to the Controller.

        req -- str that has to comply to the specified syntax and semantics of the program
        &#39;&#39;&#39;
        # checks syntax validity
        self.__interpret_request(req)
        extracted_data = pd.DataFrame()  # list where the data returned to the Controller will be stored

        self.processed_data = []  # will be the list of structured data analyzed and classified for each sub-request

        for d in self.interpreted_request:  # depending on the request src, we send the data to the various extractors
            print(self.__repr__())

            if &#39;twitter&#39; in d[&#39;src&#39;]:
                try:
                    extracted_data = requetes_twitter(d)
                except ValueError as e:
                    raise RequestSyntaxError(self.str_request, str(e))
                except Exception:
                    traceback.print_exc()
                    raise RequestSyntaxError(self.str_request, &#34;An unexpected error occurred with your Twitter search.&#34;)

            elif &#39;imdb&#39; in d[&#39;src&#39;]:
                try:
                    extracted_data = requetes_imdb(d, self.imdb_webdriver)
                except ValueError as ve:  # in the case the ID doesn&#39;t exist
                    raise RequestSyntaxError(self.str_request, str(ve))
                except Exception as e:
                    # catches the exceptions thrown by the imdb extractor. At least two are needed
                    #    - when the user needs to specify a year
                    #    - when the user needs to specify an imdb id because specifying a year wasn&#39;t enough
                    traceback.print_exc()
                    raise RequestSyntaxError(self.str_request, &#34;An unexpected error occurred with your IMDb search.&#34;)

            elif &#39;local&#39; in d[&#39;src&#39;]:
                try:
                    extracted_data = self.local_data.extract_data(d)  # extract rows from the loaded dataset
                except ValueError as e:  #  in the case no row was found
                    raise RequestSyntaxError(self.str_request, e)

            print(&#34;Result from &#34; + d[&#39;src&#39;][0] + &#34; extraction:\n&#34; + str(extracted_data.head()))

            if &#39;src_lang&#39; in extracted_data:  # if we were able to retrieve detect languages
                per_lang_extracted_data = []
                for lang in d[&#39;lang&#39;]:
                    per_lang_data = extracted_data.loc[extracted_data[&#39;src_lang&#39;] == lang]  # divides the dataset by lang
                    if not per_lang_data.empty:
                        if &#39;type&#39; in d:
                            if &#39;tweets&#39; in d[&#39;type&#39;]:  # in the case those are tweets
                                polarity = self.__get_pol_twitter(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                            else:  # or reviews
                                polarity = self.__get_pol_imdb(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                        elif &#39;twitter&#39; in d[&#39;src&#39;]:
                            polarity = self.__get_pol_twitter(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                        elif &#39;imdb&#39; in d[&#39;src&#39;]:
                            polarity = self.__get_pol_imdb(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                        per_lang_data[POL_COLUMN_NAME] = np.asarray(polarity)  # add the polarity to the df
                        per_lang_extracted_data.append(per_lang_data)

                extracted_data = pd.concat(per_lang_extracted_data)  # .sort_index() (maybe it will be sorted w/o it?)

            else:  # otherwise, we use the first language, hoping it will cover most of the data
                if &#39;type&#39; in d:
                    if &#39;tweets&#39; in d[&#39;type&#39;]:  # in the case those are tweets
                        polarity = self.__get_pol_twitter(extracted_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])
                    else:  # or reviews
                        polarity = self.__get_pol_imdb(extracted_data, d[&#39;lang&#39;][0], d[&#39;colname&#39;][0])
                    extracted_data[POL_COLUMN_NAME] = np.asarray(polarity)  # add the polarity to the df
                elif &#39;twitter&#39; in d[&#39;src&#39;]:
                    polarity = self.__get_pol_twitter(per_lang_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])
                elif &#39;imdb&#39; in d[&#39;src&#39;]:
                    polarity = self.__get_pol_imdb(per_lang_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])

            for lang in d[&#39;target&#39;]:
                extracted_data = translate_df(extracted_data, d, lang)

            # we extract the frequency of the most common words and had it to the output
            request_output = RequestOutput(self.str_request, d, extracted_data, POL_COLUMN_NAME)
            self.processed_data.append(request_output)

        print(self.__repr__())
        return self.processed_data

    def import_local_data(self, path):
        self.local_data = LocalImporter(path)
        try:
            print(self.local_data.data.head())
        except AttributeError:
            print(&#34;The file could not be read hence the imported data is None.&#34;)

    def __get_pol_twitter(self, extracted_data, language, colname):
        # wrapper method to lighten the code for language choice
        try:  # in the event no model was loaded
            if language == &#39;fr&#39;:
                return self.vc_fr_twitter.classify(extracted_data, colname, pos_max_thresh=0.7, neg_max_thresh=0.2)
            if language == &#39;en&#39;:
                return self.vc_en_twitter.classify(extracted_data, colname, pos_max_thresh=0.7, neg_max_thresh=0.2)
        except AttributeError as ae:
            raise RequestSyntaxError(self.str_request, str(ae))

    def __get_pol_imdb(self, extracted_data, language, colname):
        # wrapper method to lighten the code for language choice
        try:  # in the event no model was loaded
            if language == &#39;fr&#39;:
                return self.vc_fr_imdb.classify(extracted_data, colname, pos_max_thresh=0.7, neg_max_thresh=0.2)
            if language == &#39;en&#39;:
                return self.vc_en_imdb.classify(extracted_data, colname, pos_max_thresh=0.7, neg_max_thresh=0.2)
        except AttributeError as ae:
            raise RequestSyntaxError(self.str_request, str(ae))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="src.model.requests_handler.RequestsHandler.handle_request"><code class="name flex">
<span>def <span class="ident">handle_request</span></span>(<span>self, req)</span>
</code></dt>
<dd>
<div class="desc"><p>This method is used by the Controller to send requests to the model.
If the request is valid, it is processed by the various DataExtractors and the output data is then analyzed
and classified. Otherwise, different errors are thrown to the Controller.</p>
<p>req &ndash; str that has to comply to the specified syntax and semantics of the program</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_request(self, req):
    &#39;&#39;&#39;This method is used by the Controller to send requests to the model.
    If the request is valid, it is processed by the various DataExtractors and the output data is then analyzed
    and classified. Otherwise, different errors are thrown to the Controller.

    req -- str that has to comply to the specified syntax and semantics of the program
    &#39;&#39;&#39;
    # checks syntax validity
    self.__interpret_request(req)
    extracted_data = pd.DataFrame()  # list where the data returned to the Controller will be stored

    self.processed_data = []  # will be the list of structured data analyzed and classified for each sub-request

    for d in self.interpreted_request:  # depending on the request src, we send the data to the various extractors
        print(self.__repr__())

        if &#39;twitter&#39; in d[&#39;src&#39;]:
            try:
                extracted_data = requetes_twitter(d)
            except ValueError as e:
                raise RequestSyntaxError(self.str_request, str(e))
            except Exception:
                traceback.print_exc()
                raise RequestSyntaxError(self.str_request, &#34;An unexpected error occurred with your Twitter search.&#34;)

        elif &#39;imdb&#39; in d[&#39;src&#39;]:
            try:
                extracted_data = requetes_imdb(d, self.imdb_webdriver)
            except ValueError as ve:  # in the case the ID doesn&#39;t exist
                raise RequestSyntaxError(self.str_request, str(ve))
            except Exception as e:
                # catches the exceptions thrown by the imdb extractor. At least two are needed
                #    - when the user needs to specify a year
                #    - when the user needs to specify an imdb id because specifying a year wasn&#39;t enough
                traceback.print_exc()
                raise RequestSyntaxError(self.str_request, &#34;An unexpected error occurred with your IMDb search.&#34;)

        elif &#39;local&#39; in d[&#39;src&#39;]:
            try:
                extracted_data = self.local_data.extract_data(d)  # extract rows from the loaded dataset
            except ValueError as e:  #  in the case no row was found
                raise RequestSyntaxError(self.str_request, e)

        print(&#34;Result from &#34; + d[&#39;src&#39;][0] + &#34; extraction:\n&#34; + str(extracted_data.head()))

        if &#39;src_lang&#39; in extracted_data:  # if we were able to retrieve detect languages
            per_lang_extracted_data = []
            for lang in d[&#39;lang&#39;]:
                per_lang_data = extracted_data.loc[extracted_data[&#39;src_lang&#39;] == lang]  # divides the dataset by lang
                if not per_lang_data.empty:
                    if &#39;type&#39; in d:
                        if &#39;tweets&#39; in d[&#39;type&#39;]:  # in the case those are tweets
                            polarity = self.__get_pol_twitter(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                        else:  # or reviews
                            polarity = self.__get_pol_imdb(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                    elif &#39;twitter&#39; in d[&#39;src&#39;]:
                        polarity = self.__get_pol_twitter(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                    elif &#39;imdb&#39; in d[&#39;src&#39;]:
                        polarity = self.__get_pol_imdb(per_lang_data, lang, colname=d[&#39;colname&#39;][0])
                    per_lang_data[POL_COLUMN_NAME] = np.asarray(polarity)  # add the polarity to the df
                    per_lang_extracted_data.append(per_lang_data)

            extracted_data = pd.concat(per_lang_extracted_data)  # .sort_index() (maybe it will be sorted w/o it?)

        else:  # otherwise, we use the first language, hoping it will cover most of the data
            if &#39;type&#39; in d:
                if &#39;tweets&#39; in d[&#39;type&#39;]:  # in the case those are tweets
                    polarity = self.__get_pol_twitter(extracted_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])
                else:  # or reviews
                    polarity = self.__get_pol_imdb(extracted_data, d[&#39;lang&#39;][0], d[&#39;colname&#39;][0])
                extracted_data[POL_COLUMN_NAME] = np.asarray(polarity)  # add the polarity to the df
            elif &#39;twitter&#39; in d[&#39;src&#39;]:
                polarity = self.__get_pol_twitter(per_lang_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])
            elif &#39;imdb&#39; in d[&#39;src&#39;]:
                polarity = self.__get_pol_imdb(per_lang_data, d[&#39;lang&#39;][0], colname=d[&#39;colname&#39;][0])

        for lang in d[&#39;target&#39;]:
            extracted_data = translate_df(extracted_data, d, lang)

        # we extract the frequency of the most common words and had it to the output
        request_output = RequestOutput(self.str_request, d, extracted_data, POL_COLUMN_NAME)
        self.processed_data.append(request_output)

    print(self.__repr__())
    return self.processed_data</code></pre>
</details>
</dd>
<dt id="src.model.requests_handler.RequestsHandler.import_local_data"><code class="name flex">
<span>def <span class="ident">import_local_data</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_local_data(self, path):
    self.local_data = LocalImporter(path)
    try:
        print(self.local_data.data.head())
    except AttributeError:
        print(&#34;The file could not be read hence the imported data is None.&#34;)</code></pre>
</details>
</dd>
<dt id="src.model.requests_handler.RequestsHandler.load_classifiers"><code class="name flex">
<span>def <span class="ident">load_classifiers</span></span>(<span>self, multithreading=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_classifiers(self, multithreading=True):
    time.sleep(0.1)  # this line allows the time for the GUI to load correctly
    if multithreading:  # with multithreading
        vcs = [(self.vc_en_twitter, get_twitter_en_vc),
               (self.vc_fr_twitter, get_twitter_fr_vc),
               (self.vc_en_imdb, get_imdb_short_reviews_vc),
               (self.vc_fr_imdb, get_imdb_allocine_vc)]

        threads = []

        for vc, get in vcs:
            t = Thread(target=get, args=[vc])
            t.start()
            threads.append(t)

        for t in threads:
            t.join()

    else:  # without multithreading
        get_twitter_en_vc(self.vc_en_twitter)
        get_imdb_short_reviews_vc(self.vc_en_imdb)
        get_twitter_fr_vc(self.vc_fr_twitter)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.model" href="index.html">src.model</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.model.requests_handler.RequestSyntaxError" href="#src.model.requests_handler.RequestSyntaxError">RequestSyntaxError</a></code></h4>
</li>
<li>
<h4><code><a title="src.model.requests_handler.RequestsHandler" href="#src.model.requests_handler.RequestsHandler">RequestsHandler</a></code></h4>
<ul class="">
<li><code><a title="src.model.requests_handler.RequestsHandler.handle_request" href="#src.model.requests_handler.RequestsHandler.handle_request">handle_request</a></code></li>
<li><code><a title="src.model.requests_handler.RequestsHandler.import_local_data" href="#src.model.requests_handler.RequestsHandler.import_local_data">import_local_data</a></code></li>
<li><code><a title="src.model.requests_handler.RequestsHandler.load_classifiers" href="#src.model.requests_handler.RequestsHandler.load_classifiers">load_classifiers</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>